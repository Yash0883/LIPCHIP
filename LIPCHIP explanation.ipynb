{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3b4424-1a45-4d73-b488-d6840e21ca4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(f\"The vocabulary is: {char_to_num.get_vocabulary()} (size ={char_to_num.vocabulary_size()})\")\\nThis prints out:\\n\\nThe list of characters in the vocabulary, showing how each character is mapped to a unique integer.\\nThe size of the vocabulary, i.e., how many unique characters are in the list.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IPython display (display): Useful for controlling how outputs are shown, especially in notebook environments.\n",
    "    \n",
    "#Jiwer (wer): For calculating the word error rate, often used in evaluating speech-to-text models.\n",
    "\n",
    "#data_url = This line stores the URL of the LJSpeech dataset, which contains audio recordings and their corresponding transcriptions.\n",
    "#It's a popular dataset for speech-to-text tasks.\n",
    "#data_path have downloaded the database from the link in data_url and saved it in local machine \n",
    "#keras.utils.get_file-->A Keras utility that downloads a file\n",
    "#LJSpeech-1.1-->The name of the file after it's downloaded.\n",
    "#untar=True-->This extracts the .tar.bz2 file automatically after downloading it.\n",
    "#wavs_path = data_path + \"/wavs/\" -->This line creates a path that points to the folder containing the audio files\n",
    "# .wav format from the LJSpeech dataset. The audio files are stored in the (wavs) directory after extraction.\n",
    "# metadata_path = data_path + \"/metadata.csv\" --> This creates the path to the metadata file, which contains details about each audio file, like the transcription and file names. \n",
    "#The metadata file is named metadata.csv and is located in the dataset folder.\n",
    "#metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)-->This line reads the metadata CSV file into a Pandas DataFrame (like a table in Excel).\n",
    "#metadata_path: The path to the CSV file.\n",
    "#sep=\"|\": Specifies that the values in the file are separated by a vertical bar (|), not a comma.\n",
    "#header=None: Means the CSV file doesn't have a header row, so column names will be added manually later.\n",
    "#quoting=3: Prevents issues with quotes inside the file, which may otherwise cause problems when reading it.\n",
    "#metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
    "#Here, you're giving the DataFrame columns more meaningful names:\n",
    "#file_name: The name of the audio file (without the .wav extension).\n",
    "#transcription: The original transcription (what was said in the audio).\n",
    "#normalized_transcription: A cleaned-up version of the transcription, making it more standardized.\n",
    "#metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)-->This shuffles the rows of the DataFrame randomly\n",
    "#frac=1: Indicates that you want to shuffle the entire dataset.\n",
    "# reset_index(drop=True): This resets the index after shuffling, so you get a clean index from 0 onwards without keeping the old index.\n",
    "# \n",
    "\n",
    "\n",
    "#split = int(len(metadata_df) * 0.90)\n",
    "#This calculates the number of rows that should go into the training set, which is 90% of the total data.\n",
    "\n",
    "#len(metadata_df): This gives the total number of rows in the dataset.\n",
    "#0.90: You want 90% of the data for training.\n",
    "#int(...): Converts the result to an integer, since you can't have partial rows.\n",
    "\n",
    "#df_train = metadata_df[:split]\n",
    "#This selects the first 90% of the data for the training set.\n",
    "\n",
    "#metadata_df[:split]: This takes all rows from the start up to the index value split (the 90% mark).\n",
    "\n",
    "#df_val = metadata_df[split:]\n",
    "#This selects the remaining 10% of the data for the validation set\n",
    "\n",
    "#metadata_df[split:]: This takes all rows from the split index to the end of the dataset\n",
    "\n",
    "#print(f\"Size of the training set: {len(df_train)}\")\n",
    "#This prints the number of rows in the training set.\n",
    "\n",
    "#print(f\"Size of the validation set: {len(df_val)}\")\n",
    "#This prints the number of rows in the validation set..\n",
    "\n",
    "#In simple terms: you're splitting the dataset into two parts, \n",
    "#using 90% for training the model and 10% for validating (checking) how well the model performs.\n",
    "\n",
    "#characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
    "#This creates a list of all characters that will be used in the transcription\n",
    "\n",
    "#The list comprehension [x for x in ...] simply converts each character in the string into a list of individual characters.\n",
    "\n",
    "#char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
    "#This line maps each character to a unique integer.\n",
    "\n",
    "#keras.layers.StringLookup: A Keras layer that converts strings (characters in this case) into numerical values (integers).\n",
    "#vocabulary=characters: The list of characters that will be mapped to integers.\n",
    "#oov_token=\"\": If any character outside the defined list (vocabulary) is encountered,\n",
    "#it will be mapped to this \"Out of Vocabulary\" token (in this case, it's an empty string, meaning no special handling).\n",
    "\n",
    "\n",
    "\n",
    "#num_to_char = keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True)\n",
    "#This creates the reverse mapping—converting integers back to their corresponding characters.\n",
    "\n",
    "#invert=True: This means it will reverse the mapping, turning numbers back into characters.\n",
    "\n",
    "\n",
    "'''print(f\"The vocabulary is: {char_to_num.get_vocabulary()} (size ={char_to_num.vocabulary_size()})\")\n",
    "This prints out:\n",
    "\n",
    "The list of characters in the vocabulary, showing how each character is mapped to a unique integer.\n",
    "The size of the vocabulary, i.e., how many unique characters are in the list.'''\n",
    "\n",
    "#You are creating a way to convert text into numbers (for the model to process) and then convert those numbers back into text (to interpret the output).\n",
    "\n",
    "'''frame_length = 256\n",
    "This defines how many samples you take at once when you divide the audio into small frames for processing (like breaking a long sentence into smaller pieces).\n",
    "frame_step = 160\n",
    "This tells how many samples you move forward after each frame. It’s like how much you \"slide\" from one piece of the audio to the next.\n",
    "fft_length = 384\n",
    "This specifies the size of the Fast Fourier Transform (FFT), which is used to convert audio from a time-based signal into frequencies.\n",
    "If not set, it would automatically use the nearest power of 2 that’s bigger than frame_length.'''\n",
    "\n",
    "##def encode_single_sample(wav_file, label):\n",
    "#This is the function definition. It takes in two inputs:\n",
    "#wav_file: The name of the audio file (without the .wav extension).\n",
    "#label: The transcription or text associated with the audio.\n",
    "\n",
    "#audio, _ = tf.audio.decode_wav(file)\n",
    "#This decodes (opens and interprets) the .wav file so it can be used in TensorFlow. \n",
    "#The output is the raw audio data, and _ is the sample rate, which we don't need here.\n",
    "\n",
    "#audio = tf.squeeze(audio, axis=-1)\n",
    "#This removes unnecessary dimensions from the audio data. Audio might have an extra dimension, and this makes it a 1D signal, which is easier to work with.\n",
    "\n",
    "\n",
    "#pectrogram = tf.signal.stft(audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "#This applies Short-Time Fourier Transform (STFT) to the audio, \n",
    "#turning the 1D audio signal into a 2D spectrogram (which shows how frequencies change over time).\n",
    "\n",
    "#spectrogram = tf.abs(spectrogram)\n",
    "#Takes the magnitude (absolute value) of the spectrogram, which represents the strength of each frequency.\n",
    "\n",
    "#spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "#This reduces the range of values by taking the square root (0.5 power), making the data easier to work with.\n",
    "\n",
    "#means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "#Calculates the average (mean) of each column (frequency over time) in the spectrogram to help in normalization\n",
    "\n",
    "#stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "#Calculates the standard deviation (spread of values) for normalization.\n",
    "\n",
    "#spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
    "#This normalizes the spectrogram, making it easier for the model to learn by scaling values to have zero mean and unit variance\n",
    "#(reduces influence of large values). The 1e-10 is added to avoid division by zero.\n",
    "\n",
    "#label = tf.strings.lower(label)\n",
    "#Converts the transcription (text) to lowercase, so the model doesn't treat \"A\" and \"a\" as different characters.\n",
    "\n",
    "#label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
    "#This splits the text (label) into individual characters. It’s important because you want to process each character individually.\n",
    "\n",
    "\n",
    "#label = char_to_num(label)\n",
    "#Converts each character in the transcription into a number using the char_to_num mapping created earlier.\n",
    "\n",
    "'''return spectrogram, label\n",
    "The function returns two things:\n",
    "The processed spectrogram (a 2D representation of the audio).\n",
    "The label (the transcription converted to numbers).\n",
    "The model will use both the audio and the label during training.'''\n",
    "\n",
    "\n",
    "'''This function converts an audio file and its corresponding transcription into a format that a neural network can understand.\n",
    "It processes the audio into a spectrogram and turns the transcription into numerical values.'''\n",
    "\n",
    "\n",
    "#for batch in train_dataset.take(1):\n",
    "#This takes one batch of data from the training dataset (train_dataset). Each batch contains multiple samples of audio and their corresponding labels.\n",
    "\n",
    "#spectrogram = batch[0][0].numpy()\n",
    "#Takes the first spectrogram from the batch and converts it into a NumPy array (since TensorFlow tensors need to be converted to NumPy for visualization).\n",
    "\n",
    "#spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
    "#This trims any extra zeros from the spectrogram data and transposes it (flips rows and columns) for better visualization.\n",
    "\n",
    "#label = batch[1][0]\n",
    "#Takes the corresponding label (transcription) for the first audio sample in the batch.\n",
    "\n",
    "#label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "#Converts the label (which is a sequence of numbers) back to characters using the num_to_char mapping,\n",
    "#joins them into a single string, and decodes it to UTF-8 to get the original transcription.\n",
    "\n",
    "#file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n",
    "#Reads the first audio file from the training dataset.\n",
    "\n",
    "#audio, _ = tf.audio.decode_wav(file)\n",
    "#Decodes the .wav audio file, extracting the audio signal.\n",
    "\n",
    "#audio = audio.numpy()\n",
    "#Converts the audio tensor to a NumPy array so it can be plotted.\n",
    "\n",
    "#display.display(display.Audio(np.transpose(audio), rate=16000))\n",
    "#This plays the audio so you can hear the waveform. It uses a sampling rate of 16,000 Hz.\n",
    "\n",
    "\"\"\"The code plots both the spectrogram (a visual representation of the audio frequencies over time) and the waveform (the raw audio signal over time).\n",
    "It also plays back the audio file so you can listen to it.\n",
    "The transcription of the audio is displayed as the title of the spectrogram plot.\"\"\"\n",
    "\n",
    "'''def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
    "This function builds a DeepSpeech2-like speech recognition model. It has three main parameters:\n",
    "input_dim: The size of each input feature (the number of frequencies in the spectrogram).\n",
    "output_dim: The number of possible output classes (characters in transcription).\n",
    "rnn_layers: How many RNN (Recurrent Neural Network) layers the model will have (default is 5).\n",
    "rnn_units: The number of neurons (units) in each RNN layer (default is 128).'''\n",
    "\n",
    "\n",
    "\n",
    "'''input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
    "This defines the input layer of the model. It takes in a spectrogram \n",
    "(a 2D representation of the audio) where each time frame has input_dim features (frequencies).\n",
    "\n",
    "x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
    "The spectrogram is reshaped to add an extra dimension, making it compatible with 2D convolutional layers. \n",
    "It becomes a 3D input: time steps, features, and one channel.'''\n",
    "\n",
    "'''x = layers.Conv2D(...)\n",
    "The next two blocks are convolutional layers:\n",
    "Conv Layer 1: Takes a window of [11 x 41] from the spectrogram, moves 2 steps in each direction, and extracts 32 features.\n",
    "Conv Layer 2: Takes a smaller window of [11 x 21] with a stride of [1 x 2] to extract more refined features.\n",
    "BatchNormalization: Normalizes the data to make training faster and more stable.\n",
    "ReLU: Adds non-linearity to help the model learn complex patterns.'''\n",
    "\n",
    "\n",
    "#x = layers.Reshape(...)\n",
    "#After the convolution layers, the output is reshaped into a 2D format again so that it can be fed into RNN layers. \n",
    "#Now, the input to the RNN will be the combined feature set over time.\n",
    "\n",
    "'''for i in range(1, rnn_layers + 1):\n",
    "This loop creates multiple RNN layers (5 by default):\n",
    "GRU: A type of recurrent layer that helps in learning sequences. It processes each time step and remembers patterns over time.\n",
    "Bidirectional: It processes the input in both forward and backward directions to better capture patterns.\n",
    "Dropout: During training, some neurons are randomly turned off to prevent overfitting. This happens between RNN layers but not after the last layer.''\n",
    "\n",
    "\n",
    "''x = layers.Dense(...)\n",
    "After the RNN layers:\n",
    "A Dense layer (fully connected layer) with rnn_units * 2 neurons is added.\n",
    "ReLU activation is applied to introduce non-linearity.\n",
    "Another Dropout layer is applied to prevent overfitting.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''output = layers.Dense(units=output_dim + 1, activation=\"softmax\")\n",
    "This is the final classification layer.\n",
    "It outputs probabilities for each possible character in the transcription (including a \"blank\" token for handling the CTC loss function).'''\n",
    "\n",
    "\n",
    "\n",
    "'''model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
    "This creates the model by specifying the input and output.\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "The Adam optimizer is used for training the model, with a learning rate of 1e-4 (0.0001).\n",
    "\n",
    "model.compile(optimizer=opt, loss=CTCLoss)\n",
    "The model is compiled with the specified optimizer and CTC Loss (used for sequence-to-sequence tasks like speech recognition).'''\n",
    "\n",
    "\n",
    "\n",
    "'''model = build_model(...)\n",
    "This calls the function to build the model with the following parameters:\n",
    "\n",
    "input_dim=fft_length // 2 + 1: The size of the input spectrogram.\n",
    "output_dim=char_to_num.vocabulary_size(): The number of possible output classes (characters in transcription).\n",
    "rnn_units=512: The number of units in each RNN layer.\n",
    "model.summary(line_length=110)\n",
    "This prints a summary of the model architecture, showing the layers and the number of parameters.'''\n",
    "\n",
    "#This function builds a speech recognition model that takes in an audio spectrogram, passes it through convolutional layers (to extract features)\n",
    "#and recurrent layers (to understand sequence patterns), \n",
    "#and finally predicts the transcription using a classification layer.\n",
    "\n",
    "#predictions = [] and targets = []\n",
    "#Initializes two empty lists: predictions will store the model's outputs, and targets will hold the actual labels (ground truth) for comparison.\n",
    "\n",
    "\n",
    "'''for batch in validation_dataset:\n",
    "This loop iterates over each batch of data in the validation dataset.\n",
    "\n",
    "X, y = batch\n",
    "Unpacks the input features (X) and true labels (y) from the current batch.\n",
    "\n",
    "batch_predictions = model.predict(X)\n",
    "Uses the trained model to predict outputs based on the input features (X).\n",
    "\n",
    "batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "Decodes the model's predictions from numbers back into readable text using the decode_batch_predictions function.\n",
    "\n",
    "predictions.extend(batch_predictions)\n",
    "Adds the decoded predictions for the current batch to the overall predictions list.\n",
    "\n",
    "for label in y:\n",
    "Loops through the true labels in the current batch.\n",
    "\n",
    "label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "Converts each label from character numbers back to readable text.\n",
    "targets.append(label)\n",
    "Adds the true label text to the targets list.'''\n",
    "\n",
    "#wer_score = wer(targets, predictions)\n",
    "#Calculates the Word Error Rate (WER) by comparing the actual labels (targets) with the predicted outputs (predictions).\n",
    "#A lower score indicates better performance.\n",
    "\n",
    "'''The code evaluates the model's performance on the validation dataset by predicting the outputs and comparing them to the actual labels. \n",
    "It calculates the Word Error Rate (WER) and prints the results along with some sample predictions\n",
    "and their corresponding true labels for further analysis.'''\n",
    "\n",
    "#epochs = 1\n",
    "#This line sets the number of epochs to 1. \n",
    "#An epoch is one complete pass through the training dataset. In this case, the model will train for 1 epoch.\n",
    "\n",
    "#validation_callback = CallbackEval(validation_dataset)\n",
    "#This line creates a callback function that will be executed after each epoch during training.\n",
    "#It uses the validation dataset to check the model’s performance after every epoch by calculating the Word Error Rate (WER) and displaying sample predictions.\n",
    "\n",
    "'''model.fit() is used to train the model.\n",
    "train_dataset: The dataset the model will learn from during training.\n",
    "validation_data=validation_dataset: This is the dataset that will be used to validate the model’s performance after each epoch.\n",
    "epochs=epochs: The number of times the model will go through the training dataset. In this case, it is 1.\n",
    "callbacks=[validation_callback]: This specifies that the validation_callback function will be executed after every epoch\n",
    "to check how the model is performing on the validation dataset.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613d21c-0247-4c0c-a857-ee62473119c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
